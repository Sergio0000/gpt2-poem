{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gp2_poem_F.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMaOJPwET7+yXectPL1X01O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"euamieludIOV","executionInfo":{"status":"ok","timestamp":1626721574100,"user_tz":240,"elapsed":2672,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"b79d360f-686e-4928-a1b2-3ed791812ea4"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NdKi52ZrYywH"},"source":["# We'll need these libraries to gather and shape the data.\n","import requests \n","import pandas as pd\n","from itertools import compress\n","import re\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import numpy as np\n","import random\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import torch.nn.functional as F\n","import csv\n","import torch\n","from transformers import AutoModelWithLMHead, AutoTokenizer, AdamW, get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Saq-jvAndTnU","executionInfo":{"status":"ok","timestamp":1626721579721,"user_tz":240,"elapsed":6,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"429f19b7-9b47-4ed4-950b-1dc7c672cf5c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Iu2OsB5CvGtL"},"source":["# CargarDatos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ch08NQsteILX","executionInfo":{"status":"ok","timestamp":1626721587319,"user_tz":240,"elapsed":1002,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"98732436-2367-4b49-ff85-7019779d7024"},"source":["data_path = \"/content/drive/MyDrive/sis330/Final/dataset/poems.csv\"\n","data = pd.read_csv(data_path)\n","data.fillna(\"\", inplace=True)\n","print(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                                   poems tipo_rima\n","0      gracias está bien.\\nPorque en las tardes con s...      abab\n","1      Porque en las tardes con sutil desmayo.\\npiado...      abab\n","2      Padre viejo y triste rey de las divinas cancio...      abab\n","3      son en mi camino focos de una luz enigmática.\\...      abab\n","4      tus pupilas mustias vagas de pensar y abstracc...      abab\n","...                                                  ...       ...\n","10319  sobre aquella agua de esplendor desnuda.\\nse v...      abab\n","10320  esa ave taciturna en qué medita.\\nNo ha sacudi...      abba\n","10321  en que rasgabas el azul de enero.\\ncon tu aman...      abba\n","10322  donde con él soñabas y dormías.\\nal recio empu...      abba\n","10323  nido y amor por otras compañeras.\\ny tú cansad...      abba\n","\n","[10324 rows x 2 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mgQd2IdXeueS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeUfVZHxdEuq"},"source":["poems = data['poems'].to_list()\n","tipo_rima = data['tipo_rima'].to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLI1urUQvCHX"},"source":["# Model and Tokenizader\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5CpGFJHfGHv","executionInfo":{"status":"ok","timestamp":1626721611715,"user_tz":240,"elapsed":8562,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"7873c8c3-ef2d-462f-8248-3458189432c4"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n","                                          #,\n","                                         # bos_token='<|startoftext|>', \n","                                         # eos_token='<|endoftext|>', \n","                                         # pad_token='<|pad|>')\n","special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n","tokenizer.add_special_tokens(special_tokens_dict)\n","#okenizer.add_special_tokens({'additional_special_tokens':['<rim>','<eol>']})\n","max_tokens_poema = max([len(tokenizer.encode(poema)) for poema in poems])\n","model = AutoModelWithLMHead.from_pretrained(\"datificate/gpt2-small-spanish\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:847: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRg2F55Fdy0H","executionInfo":{"status":"ok","timestamp":1626721620253,"user_tz":240,"elapsed":438,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"f243cb48-b366-489c-d4bb-851665b04fe6"},"source":["validation_start_index = int(len(poems) * 0.2)\n","validation_start_index"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2064"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"JTag6vnJuIQG"},"source":["# class Data set"]},{"cell_type":"code","metadata":{"id":"VWNEoKcwdzD7"},"source":["class PoetryDataset(Dataset):\n","    \n","    def __init__(self, texts, tipo_rima, tokenizer, max_length):\n","        \n","        \n","        self.tokenizer = tokenizer\n","        self.texts = texts\n","        self.tipo_rimas = tipo_rima\n","\n","        self.max_len = max_length\n","\n","    def __len__(self):        \n","        return len(self.texts)\n","    \n","    def __getitem__(self, index):\n","        \n","        text = self.texts[index]\n","        tipo_rima = self.tipo_rimas[index]\n","       # text = f' <BOS>{tipo_rima} <rim> {text} <EOS> '\n","        text = f'<BOC>{tipo_rima}<EOC>{text}'\n","        enc_dict = tokenizer(text, truncation=True, max_length=self.max_len, padding=\"max_length\")\n","        tokenized = enc_dict['input_ids']\n","        \n","        return torch.tensor(tokenized).long()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9DTfoOYiwT3","executionInfo":{"status":"ok","timestamp":1626721745079,"user_tz":240,"elapsed":2,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"c1218e37-c5a5-43b3-c89b-07f3e0430d0c"},"source":["poems_train = PoetryDataset(poems[:-validation_start_index], tipo_rima[:-validation_start_index], tokenizer, max_tokens_poema)\n","poems_valid = PoetryDataset(poems[-validation_start_index:], tipo_rima[-validation_start_index:],tokenizer, max_tokens_poema)\n","\n","len(poems_train), len(poems_valid)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8260, 2064)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mOUbV6sxix50","executionInfo":{"status":"ok","timestamp":1626721746118,"user_tz":240,"elapsed":9,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"c5d9ede9-68d6-4934-d4ec-96aa9c529648"},"source":["poems_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([   28,    34, 14010,    30,   337,   337,    28,    37, 14010,    30,\n","          707,   727,   815,  1329,    14,   199,  2636,   292,   278,   347,\n","        35561,   295, 28168,   370, 22149,    14,   199,  1134,   413,   538,\n","        46438,   284,   671,  3514, 18245,    14,   199,    89,  6937,   276,\n","        21815, 24620,   295,   308, 20159,    14,   199, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaGFz_Xtlj34","executionInfo":{"status":"ok","timestamp":1626721746613,"user_tz":240,"elapsed":2,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"47c35f28-f797-492e-89de-d6771f4ae0c2"},"source":["poems_train[100]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([   28,    34, 14010,    30,   337,   526,    28,    37, 14010,    30,\n","          694,  9310,   258,   276, 13552,   301,   306, 20899,    14,   199,\n","           65,  1130, 22425, 14576,  5703, 34533,    14,   199,  2443,  9175,\n","          258,   312, 39090,    14,   199,   257, 10120,   268,   265,   298,\n","        17542,    14,   199, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n","        50257, 50257, 50257, 50257, 50257])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zt9uYdxblKP_","executionInfo":{"status":"ok","timestamp":1626721759437,"user_tz":240,"elapsed":537,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"bf881508-ce6c-44a6-9875-4f83f4b40112"},"source":["print(tokenizer.decode([ 28,    34, 14010,    30,   337,   526,    28,    37, 14010,    30,\n","          694,  9310,   258,   276, 13552,   301,   306, 20899,    14,   199,\n","           65,  1130, 22425, 14576,  5703, 34533,    14,   199,  2443,  9175,\n","          258,   312, 39090,    14,   199,   257, 10120,   268,   265,   298,\n","        17542,    14,   199, 50257]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<BOC>abba<EOC>caricias de la hembra que se plasma.\n","a todos tus deseos invencibles.\n","ese imposible de los imposibles.\n","de adorar a un fantasma.\n","<PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sSQROycKlbJc"},"source":["batch_size = 32\n","train_loader = DataLoader(poems_train,\n","                          batch_size=batch_size,\n","                              shuffle=True)\n","\n","validation_loader = DataLoader(poems_valid,\n","                            batch_size=batch_size,\n","                            shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f16x6ShouRA9"},"source":["# HyperParametros\n"]},{"cell_type":"code","metadata":{"id":"MnnLUiEcoedZ"},"source":["device = 'cuda'\n","num_epochs = 40\n","learning_rate = 0.0001\n","warmup_steps = 50\n","total_steps = len(train_loader) * num_epochs\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=warmup_steps,\n","                                            num_training_steps=total_steps)\n","model.resize_token_embeddings(len(tokenizer))\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eHI76KeuNf3"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"ZHCg4GqnBRGs"},"source":["import pandas as pd\n","from tqdm import tqdm\n","import numpy as np\n","import random\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtQZ6iyPoosL"},"source":["def train(model, loader, optimizer, scheduler, last_n_losses=200, verbose=True):\n","\n","    losses = []\n","\n","    progress_bar = tqdm(total=len(loader.dataset), disable=not verbose, desc='Train')\n","\n","    model.train()\n","\n","    for toks in loader:\n","\n","        toks = toks.to(device)\n","        \n","        outputs = model(toks,\n","                        labels=toks\n","                        )\n","        loss = outputs[0]\n","        losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]),\n","                                 perplexity=np.exp(np.mean(losses[-last_n_losses:])))\n","        \n","        progress_bar.update()\n","\n","    progress_bar.close()\n","    \n","    return losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e8b71NGowPs"},"source":["def evaluate(model, loader, last_n_losses=200, verbose=True):\n","\n","    losses = []\n","\n","    progress_bar = tqdm(total=len(loader), disable=not verbose, desc='Evaluate')\n","\n","    model.eval()\n","\n","    for toks in loader:\n","\n","        toks = toks.to(device)\n","        \n","        outputs = model(toks,\n","                        labels=toks\n","                        )\n","        loss = outputs[0]\n","        losses.append(loss.item())\n","\n","        progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]),\n","                                 perplexity=np.exp(np.mean(losses[-last_n_losses:])))\n","        \n","        progress_bar.update()\n","\n","    progress_bar.close()\n","    \n","    return losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QB2r0PfOo2lX"},"source":["\n","save_best_model_path = '/content/drive/MyDrive/sis330/Final/model/best_model_state_dict.pth'\n","save_best_optimizer_path = '/content/drive/MyDrive/sis330/Final/model/best_optimizer_state_dict.pth'\n","save_last_model_path = '/content/drive/MyDrive/sis330/Final/last_model_state_dict.pth'\n","save_last_optimizer_path = '/content/drive/MyDrive/sis330/Final/model/last_optimizer_state_dict.pth'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OTLPHa28bPT"},"source":["home_dir = '/content/drive/MyDrive/sis330/Final/model_gpt2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sygRR3OJpEmj","executionInfo":{"status":"ok","timestamp":1626722194176,"user_tz":240,"elapsed":399134,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"6646c7d4-41dd-417f-9d5e-dcaa008ef8a4"},"source":["train_losses = []\n","validation_losses = []\n","\n","train_perplexities = []\n","validation_perplexities = []\n","\n","best_validation_loss = 1e+6\n","\n","for n_epoch in range(1, num_epochs + 1):\n","    \n","    epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n","    epoch_validation_losses = evaluate(model, validation_loader)\n","    \n","    mean_train_loss = np.mean(epoch_train_losses)\n","    mean_validation_loss = np.mean(epoch_validation_losses)\n","    \n","    train_losses.extend(epoch_train_losses)\n","    train_perplexities.append(np.exp(mean_train_loss))\n","    \n","    validation_losses.extend(epoch_validation_losses)\n","    validation_perplexities.append(np.exp(mean_validation_loss))\n","    \n","    message = f'Epoch: {n_epoch}\\n'\n","    message += f'Train: loss - {mean_train_loss:.4f} | perplexity - {train_perplexities[-1]:.3f}\\n'\n","    message += f'Validation: loss - {mean_validation_loss:.4f} | perplexity - {validation_perplexities[-1]:.3f}'\n","    \n","    print(message)\n","    \n","    if mean_validation_loss < best_validation_loss:\n","        \n","        best_validation_loss = mean_validation_loss\n","        \n","        torch.save(model.state_dict(), save_best_model_path)\n","        torch.save(optimizer.state_dict(), save_best_optimizer_path)\n","        \n","    else:\n","        break\n","        \n","    torch.save(model.state_dict(), save_last_model_path)\n","    torch.save(optimizer.state_dict(), save_last_optimizer_path)\n","\n","    with open(home_dir + f'info_{n_epoch}.json', 'w') as file_object:\n","\n","        info = {\n","            'message': message,\n","            'train_losses': train_losses,\n","            'validation_losses': validation_losses,\n","            'train_perplexities': train_perplexities,\n","            'validation_perplexities': validation_perplexities\n","        }\n","\n","        file_object.write(json.dumps(info, indent=2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train:   3%|▎         | 259/8260 [01:47<55:18,  2.41it/s, loss=2.21, perplexity=9.16]\n","Evaluate: 100%|██████████| 65/65 [00:09<00:00,  6.78it/s, loss=2.1, perplexity=8.14]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1\n","Train: loss - 2.3900 | perplexity - 10.914\n","Validation: loss - 2.0974 | perplexity - 8.145\n"],"name":"stdout"},{"output_type":"stream","text":["Train:   3%|▎         | 259/8260 [01:47<55:20,  2.41it/s, loss=1.97, perplexity=7.18]\n","Evaluate: 100%|██████████| 65/65 [00:09<00:00,  6.80it/s, loss=2.06, perplexity=7.87]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2\n","Train: loss - 1.9825 | perplexity - 7.261\n","Validation: loss - 2.0633 | perplexity - 7.872\n"],"name":"stdout"},{"output_type":"stream","text":["Train:   3%|▎         | 259/8260 [01:47<55:20,  2.41it/s, loss=1.83, perplexity=6.21]\n","Evaluate: 100%|██████████| 65/65 [00:09<00:00,  6.80it/s, loss=2.08, perplexity=7.98]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3\n","Train: loss - 1.8299 | perplexity - 6.233\n","Validation: loss - 2.0775 | perplexity - 7.985\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"GPnHVDS0vTzs"},"source":["# save Model\n"]},{"cell_type":"code","metadata":{"id":"9BjQ1xN-EgNc"},"source":["torch.save(model.state_dict(), save_best_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MxWVLxMZwKll"},"source":["# generaciones de salida"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbrBEQ6PFQhV","executionInfo":{"status":"ok","timestamp":1626730801264,"user_tz":240,"elapsed":27120,"user":{"displayName":"Juan Sergio Pita Colque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir9-uRSKwRDw8wTtQPtlDpXyGfJM0AqNPPKu4L=s64","userId":"12788953723665597065"}},"outputId":"6e3922e8-9a99-438d-8168-251400e5b6bd"},"source":["def generate(\n","    model,\n","    tokenizer,\n","    abba,\n","    prompt,\n","    entry_count=1,\n","    entry_length=100, #maximum number of words\n","    top_p=0.9,\n","    temperature=1.,\n","):\n","\n","    model.eval()\n","\n","    generated_num = 0\n","    generated_list = []\n","\n","    filter_value = -float(\"Inf\")\n","\n","    with torch.no_grad():\n","\n","        for entry_idx in trange(entry_count):\n","\n","            entry_finished = False\n","\n","            #generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","           # generated = generated.to(device)\n","           # print(generated)\n","            prompt = f'[BOC]{abba}[EOC]{prompt}'\n","            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","            print(len(generated))\n","            for i in range(entry_length):\n","                  \n","                outputs = model(generated, labels=generated)\n","               # print(outputs)\n","                loss, logits = outputs[:2]\n","                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n","                    ..., :-1\n","                ].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = filter_value\n","\n","                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n","                generated = torch.cat((generated, next_token), dim=1)\n","\n","                if next_token in tokenizer.encode(\"<EOS>\"):\n","                    entry_finished = True\n","\n","                if entry_finished:\n","                    \n","                    next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n","                   # print('nt',next_token)\n","                    generated_num = generated_num + 1\n","                  #  print('g',generated)\n","                    output_list = list(generated.squeeze().numpy())\n","                    print(tokenizer.decode(output_list))\n","                    output_text = tokenizer.decode(output_list)\n","                    output_text = output_text.strip(f'[BOC]{abba}[EOC]')\n","                    output_text = output_text.strip(f'<EOS>')\n","                    generated_list.append(output_text)\n","                    break\n","            \n","            if not entry_finished:\n","              output_list = list(generated.squeeze().numpy())\n","              output_text = f\"{tokenizer.decode(output_list)}<EOS>\" \n","              generated_list.append(output_text)\n","    return generated_list\n","\n","sentence3=generate(model.to('cpu'), tokenizer, \"abba\",\"la muerte llego antes de lo previsto\")\n","versos3=sentence3[0].split('.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:26<00:00, 26.69s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qWt7iwczJNrM"},"source":["def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n","\n","    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n","    top_k = min(top_k, logits.size(-1))  # Safety check\n","    if top_k > 0:\n","        # Remove all tokens with a probability less than the last token of the top-k\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = filter_value\n","    return logits\n","from tqdm import tnrange as trange\n","from tqdm import trange\n","\n","\n","def sample_seq(model,abba, context, length, device=\"cpu\", temperature=1, top_k=0, top_p=0.0):\n","\n","    prompt = f'<BOC>{abba}<EOC>{context}'\n","    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","  #  generated = context\n","    with torch.no_grad():  \n","        for _ in range(length):\n","            inputs = {'input_ids': generated}\n","            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n","            next_token_logits = outputs[0][0, -1, :] / temperature\n","            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n","            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n","            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n","    return generated\n","sentence=sample_seq(model,\"abba\",\"\",100)\n","sentence=tokenizer.decode(sentence[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZA8lkbBoasg"},"source":[""],"execution_count":null,"outputs":[]}]}